{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# 完整的训练语言模型\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA = 'ptb.train'\n",
    "TEST_DATA = 'ptb.test'\n",
    "EVAL_DATA = 'ptb.valid'\n",
    "HIDDEN_SIZE  = 300\n",
    "NUM_LAYERS = 2 # LSTM 结构的层数\n",
    "VOCAB_SIZE = 10000 # 字典规模\n",
    "TRAIN_BATCH_SIZE = 20 # 训练数据batch大小\n",
    "TRAIN_NUM_STEP = 35 # 训练数据阶段长度\n",
    "\n",
    "EVAL_BATCH_SIZE = 1 # 测试数据batch大小\n",
    "EVAL_NUM_STEP = 1 # 测试数据阶段长度\n",
    "NUM_EPOCH = 5 # 使用训练数据轮数\n",
    "LSTM_KEEP_PROB = 0.9\n",
    "EMBEDDING_KEEP_PROB = 0.9\n",
    "MAX_GRAD_NORM = 5 # 用于控制梯度膨胀的梯度大小上限\n",
    "SHARED_EMB_AND_SOFTMAX = True # 在softmax层和词向量层之间共享参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通过一个PTBModel类来描述模型\n",
    "class PTBModel(object):\n",
    "    def __init__(self, is_training, batch_size, num_steps):\n",
    "        '''\n",
    "        Arg:\n",
    "            is_training: 表示是否在训练\n",
    "            batch_size: 表示batch size\n",
    "            num_stpes: 表示截断长度\n",
    "        '''\n",
    "        self.batch_size = batch_size\n",
    "        self.num_steps = num_steps\n",
    "        self.input_data = tf.placeholder(tf.int32, [batch_size, num_steps])\n",
    "        self.targets = tf.placeholder(tf.int32, [batch_size, num_steps])\n",
    "        \n",
    "        # 构造LSTM结构，多层的LSTM，包括了dropout机制\n",
    "        dropout_keep_prob = LSTM_KEEP_PROB if is_training else 1.0\n",
    "        lstm_cells = [\n",
    "            tf.nn.rnn_cell.DropoutWrapper(\n",
    "            tf.nn.rnn_cell.BasicLSTMCell(HIDDEN_SIZE),\n",
    "                output_keep_prob = dropout_keep_prob\n",
    "            ) for _ in range(NUM_LAYERS)\n",
    "        ]\n",
    "        # 多层结构\n",
    "        cell = tf.nn.rnn_cell.MultiRNNCell(lstm_cells)\n",
    "        \n",
    "        # 表示初始状态\n",
    "        self.initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "        \n",
    "        # 定义单词的词向量矩阵\n",
    "        embedding = tf.get_variable(\"embedding\", [VOCAB_SIZE, HIDDEN_SIZE])\n",
    "        \n",
    "        # 将数据转化为词向量表示\n",
    "        inputs = tf.nn.embedding_lookup(embedding, self.input_data)\n",
    "        \n",
    "        # 只在训练时使用dropout\n",
    "        if is_training:\n",
    "            inputs = tf.nn.dropout(inputs, EMBEDDING_KEEP_PROB)\n",
    "        \n",
    "        # 定义输出列表，先将不同时刻LSTM结构的输出收集起来，再一起提供给softmax层\n",
    "        outputs = []\n",
    "        state = self.initial_state\n",
    "        with tf.variable_scope('RNN'):\n",
    "            for time_step in range(num_steps):\n",
    "                if time_step > 0:\n",
    "                    tf.get_variable_scope().reuse_variables()\n",
    "                cell_output, state = cell(inputs[:,time_step,:], state) # 每次计算每个time_step的结果\n",
    "                outputs.append(cell_output)\n",
    "                \n",
    "        # 把输出队列展开成[batch, hidden_size * num_steps]的形状，然后再reshape成\n",
    "        output = tf.reshape(tf.concat(outputs, 1),[-1, HIDDEN_SIZE])\n",
    "        \n",
    "        # softmax层：将RNN在每个位置上的输出转化为各个单词的logits\n",
    "        if SHARED_EMB_AND_SOFTMAX:\n",
    "            weight = tf.transpose(embedding)\n",
    "        else:\n",
    "            weight = tf.get_variable(\"weight\", [HIDDEN_SIZE, VOCAB_SIZE])\n",
    "            \n",
    "        bias = tf.get_variable(\"bias\", [VOCAB_SIZE])\n",
    "        \n",
    "        logits = tf.matmul(output, weight) + bias\n",
    "        \n",
    "        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            labels = tf.reshape(self.targets, [-1]),\n",
    "            logits = logits\n",
    "        )\n",
    "        self.cost = tf.reduce_sum(loss) / batch_size\n",
    "        self.final_state = state\n",
    "        \n",
    "        # 如果是训练状态，那么还需要实现反向传播\n",
    "        if not is_training:\n",
    "            return\n",
    "        \n",
    "        # 这个训练过程没有学过\n",
    "        trainable_variables = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(self.cost, trainable_variables), MAX_GRAD_NORM)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate = 1.0)\n",
    "        self.train_op = optimizer.apply_gradients(\n",
    "            zip(grads, trainable_variables)\n",
    "        )\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(session, model, batches, train_op, output_log, step):\n",
    "    '''\n",
    "    训练函数\n",
    "    Args:\n",
    "        session 上下文\n",
    "        model 上述模型的实例\n",
    "        batches 数据\n",
    "        train_op\n",
    "        output_log 是否打印输出日志\n",
    "        step\n",
    "    Return:\n",
    "        step\n",
    "        结果\n",
    "    '''\n",
    "    total_costs = 0.0\n",
    "    iters = 9\n",
    "    state = session.run(model.initial_state)\n",
    "    \n",
    "    # 训练一个epoch\n",
    "    for x,y in batches:\n",
    "        # 在当前batch上运行train_op 并计算损失值\n",
    "        cost, state, _ = session.run([model.cost, model.final_state, train_op],\n",
    "                                  feed_dict = {model.input_data: x, model.targets: y, model.initial_state: state}\n",
    "                                 )\n",
    "        total_costs += cost\n",
    "        iters += model.num_steps # 迭代次数\n",
    "        \n",
    "        # 在训练时输出日志\n",
    "        if output_log and step % 100 == 0:\n",
    "            print (\"After %d steps, perplexity is % .3f\" % (step, np.exp(total_costs / iters)))\n",
    "        step += 1\n",
    "    \n",
    "    return step, np.exp(total_costs / iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file_path):\n",
    "    '''\n",
    "    读取数据，返回包含单词编号的数组，一整个文本的内容作为一个数组返回，每行句子拼接起来\n",
    "    '''\n",
    "    with open(file_path, 'r') as fin:\n",
    "        id_string = ' '.join([line.strip() for line in fin.readlines()])\n",
    "    id_list = [int(w) for w in id_string.split()]\n",
    "    return id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batches(id_list, batch_size, num_step):\n",
    "    '''\n",
    "    获取到batch\n",
    "    Args:\n",
    "        id_list: 一整个文本组成的数组，内容是word的id\n",
    "        batch_size: batch的大小\n",
    "        num_step: 表示训练时的上下文，输入的单词个数\n",
    "    '''\n",
    "    num_batches = (len(id_list) - 1) // (batch_size * num_step) # batch的数量\n",
    "    print num_batches\n",
    "    data = np.array(id_list[:num_batches * batch_size * num_step])\n",
    "    print data.shape\n",
    "    data = np.reshape(data, [batch_size, num_batches * num_step]) # 将数据切分成 batch_size, num_batches * num_steps的数组\n",
    "    # 沿着第二个维度将数据切分为num_batches的batch，存入一个数组\n",
    "    print data.shape\n",
    "    data_batches = np.split(data, num_batches, axis = 1)\n",
    "    print data_batches[0].shape\n",
    "    \n",
    "    label = np.array(id_list[1:num_batches * batch_size * num_step + 1])\n",
    "    label = np.reshape(label, [batch_size, num_batches * num_step])\n",
    "    label_batches = np.split(label, num_batches, axis=1)\n",
    "    return list(zip(data_batches, label_batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    initializer = tf.random_uniform_initializer(-0.05, 0.05) # 初始化函数\n",
    "    \n",
    "    # 训练的RNN模型\n",
    "    with tf.variable_scope(\"language_model\", reuse=None, initializer=initializer):\n",
    "        train_model = PTBModel(is_training=True, batch_size=TRAIN_BATCH_SIZE, num_steps=TRAIN_NUM_STEP)\n",
    "    \n",
    "    \n",
    "    # 测试用的模型，与train_model共用参数，但没有dropout(is_training=False)\n",
    "    with tf.variable_scope(\"language_model\", reuse=True, initializer=initializer):\n",
    "        eval_model = PTBModel(is_training=False, batch_size=EVAL_BATCH_SIZE, num_steps=EVAL_NUM_STEP)\n",
    "    \n",
    "    # 训练模型\n",
    "    with tf.Session() as sess:\n",
    "        # 初始化\n",
    "        tf.global_variables_initializer().run()\n",
    "        \n",
    "        # 训练数据\n",
    "        train_batches = make_batches(\n",
    "            read_data(TRAIN_DATA),\n",
    "            TRAIN_BATCH_SIZE,\n",
    "            TRAIN_NUM_STEP\n",
    "        )\n",
    "        \n",
    "        # eval数据\n",
    "        eval_batches = make_batches(\n",
    "            read_data(EVAL_DATA),\n",
    "            EVAL_BATCH_SIZE,\n",
    "            EVAL_NUM_STEP\n",
    "        )\n",
    "        \n",
    "        # test数据\n",
    "        test_batches = make_batches(\n",
    "            read_data(TEST_DATA),\n",
    "            EVAL_BATCH_SIZE,\n",
    "            EVAL_NUM_STEP\n",
    "        )\n",
    "        \n",
    "        step = 0\n",
    "        # 每一轮\n",
    "        for i in range(NUM_EPOCH):\n",
    "            print (\"In iteration: %d\" % (i + 1))\n",
    "            # 训练过程\n",
    "            step, train_pplx = run_epoch(sess, train_model, train_batches, \n",
    "                                         train_model.train_op, \n",
    "                                         output_log = True,\n",
    "                                         step = step\n",
    "                                        )\n",
    "            print (\"Epoch: %d Train Perplexity: %.3f\" % (i+1, train_pplx))\n",
    "            \n",
    "            # evaluation过程\n",
    "            _, eval_pplx = run_epoch(sess, eval_model, eval_batches, \n",
    "                                     tf.no_op(), \n",
    "                                     output_log = False,\n",
    "                                     step = 0\n",
    "                                    )\n",
    "            print (\"Epoch: %d Eval Perplexity: %.3f\" % (i + 1, eval_pplx))\n",
    "        _, test_pplx = run_epoch(sess, eval_model, eval_batches, tf.no_op(),output_log = False, step = 0)\n",
    "        # 训练结束进行test\n",
    "        print (\"Test Perplexity: %.3f\" % (test_pplx))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1327\n",
      "(928900,)\n",
      "(20, 46445)\n",
      "(20, 35)\n",
      "73759\n",
      "(73759,)\n",
      "(1, 73759)\n",
      "(1, 1)\n",
      "82429\n",
      "(82429,)\n",
      "(1, 82429)\n",
      "(1, 1)\n",
      "In iteration: 1\n",
      "After 0 steps, perplexity is  1521.277\n",
      "After 100 steps, perplexity is  1700.396\n",
      "After 200 steps, perplexity is  1152.936\n",
      "After 300 steps, perplexity is  905.381\n",
      "After 400 steps, perplexity is  741.359\n",
      "After 500 steps, perplexity is  631.931\n",
      "After 600 steps, perplexity is  559.752\n",
      "After 700 steps, perplexity is  504.281\n",
      "After 800 steps, perplexity is  455.512\n",
      "After 900 steps, perplexity is  419.134\n",
      "After 1000 steps, perplexity is  393.090\n",
      "After 1100 steps, perplexity is  366.900\n",
      "After 1200 steps, perplexity is  346.496\n",
      "After 1300 steps, perplexity is  326.839\n",
      "Epoch: 1 Train Perplexity: 323.761\n",
      "Epoch: 1 Eval Perplexity: 183.969\n",
      "In iteration: 2\n",
      "After 1400 steps, perplexity is  174.296\n",
      "After 1500 steps, perplexity is  162.867\n",
      "After 1600 steps, perplexity is  165.551\n",
      "After 1700 steps, perplexity is  162.668\n",
      "After 1800 steps, perplexity is  158.221\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
